{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yaoji\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yaoji\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForQuestionAnswering(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "model.load_state_dict(torch.load('../model/model_weights_6.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('../model/qa_model_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../model/qa_tokenizer_final\\\\tokenizer_config.json',\n",
       " '../model/qa_tokenizer_final\\\\special_tokens_map.json',\n",
       " '../model/qa_tokenizer_final\\\\vocab.txt',\n",
       " '../model/qa_tokenizer_final\\\\added_tokens.json',\n",
       " '../model/qa_tokenizer_final\\\\tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('../model/qa_tokenizer_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How many people worldwide are currently addicted to drugs??\"\n",
    "context = \"More than 35 million people worldwide are currently addicted to drugs. COVID-19 has created new routes for drug trafficking that increase the risk of drug addiction, making it vital to address this problem. The aim is to effectively protect the physical health of PWUD and prevent the combination of COVID-19 and the physiological and psychological effects of drugs from affecting relapse behaviour.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../clean.csv')\n",
    "df_papers = df.groupby('paper_id')['text'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'according to current live statistics at the time of editing this letter, Russia has been the third country in the world to be affected by COVID-19 with both new cases and death rates rising. it remains in a position of advantage due to the later onset of the viral spread within the country since the worldwide disease outbreak.the first step in fighting the epidemic was nationwide lock down on March 30 th , 2020.most of the multidisciplinary hospitals have been repurposed as dedicated COVID-19 centres, so the surgeons started working as infectious disease specialists. such a reallocation of health care capacity results in the effective management of this epidemiological problem 1 . the staff has undergone on-line 36-hour training course to become qualified in coronavirus infection treatment.the surgeons of COVID-19 dedicated hospitals do rarely practice surgery. when ICU patients need mechanical ventilation, percutaneous tracheostomy under endoscopic control is mostly performed, as it decreases the aerosol formation, viral load on staff and complications, associated with an endotracheal tube in comparison with surgical tracheostomy 2 . however, it is still associated with the risk of aerosol formation, so different approaches should be considered for a long-time perspective 3 .the majority of the studies dedicated to colorectal diseases are temporarily paused. the teaching and training are mostly translated via online platforms, which has excluded the opportunity to get clinical experience in surgery 4 .the approach to patient routing has changed significantly. if one is not diagnosed with COVID-19 CT scan and laboratory testing are provided immediately. the patients should be admitted to the surgical department, where treatment is provided only to those COVID-19 negative.the patient isolated for more than 2 weeks and COVID-19 negative as a result of 2 subsequent tests is admitted to the surgical department with an option to readmission to the infectious department and can be treated by surgical staff, which does not work with COVID-19 positive patients.the patient, diagnosed with coronavirus infection and treated at home is admitted to COVID-19 dedicated multidisciplinary hospital, where surgical care is provided. those treated in infectious diseases hospital or COVID-19 dedicated centre are managed by the surgical team present.surgery has become highly elective, being mostly available for high-risk patients with emergencies, malignancies, cardiovascular pathologies or infections. preoperative testing in surgical patients with respiratory symptoms and history of travelling or contacting with COVID-19 positive people and postoperative recovery in the operating unit seem to be highly effective measures 5 . a lot of rearrangements are performed locally regarding personal protective equipment, the organization of scrubbing, donning and doffing, and dedicated changing areas. moreover, observational departments are organized in surgical hospitals for patient allocation before coronavirus infection status is defined 6 .surgery for benign disorders, precancerous lesions, and reconstructive procedures are currently postponed. regarding colorectal cancer, surgical treatment may be postponed, if it is a non-obstructing disease 7 .laparoscopic surgery and diathermy are limited as well. the importance of special operating theatre for COVID-19 patients with negative pressure ventilation, unidirectional laminar flow, as well as the use of smoke evacuation systems during surgery are taken into account 8 .such an electiveness of surgery is concerning, as it might cause a worldwide healthcare catastrophe in the post-pandemic era 5 . more efforts should be taken to expand the amount and types of surgical procedures performed.due to the early preventive and corrective actions we have already reached the plateau in new cases curve, counting for up to 8 984 cases identified at the time of writing this paper June 7 th , 2020, with a mortality rate of 15075. these statistical outcomes are resulted by a 68-day lockdown, admission regime, and healthcare rearrangement. thus, the multistep restriction lifting has already started to consistently recover in both social and economic aspects.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_papers[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = df_papers[\"text\"][0]\n",
    "question = \"What is the surgery that might cause a worldwide healthcare catastrophe in the post-pandemic era 5?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.034822408109903336, 'start': 52, 'end': 60, 'answer': 'addicted'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "question_answerer(question=question, context=context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
